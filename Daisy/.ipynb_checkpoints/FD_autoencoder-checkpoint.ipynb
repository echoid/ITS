{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import NumericalField, CategoricalField, Iterator\n",
    "from data import Dataset\n",
    "from synthesizer import VGAN_generator, VGAN_discriminator\n",
    "from synthesizer import LGAN_generator, LGAN_discriminator, LSTM_discriminator\n",
    "from synthesizer import DCGAN_generator, DCGAN_discriminator\n",
    "from synthesizer import V_Train\n",
    "from random import choice\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGAN_variable = {\n",
    "\t\"batch_size\":[64],\n",
    "\t\"z_dim\":[128],\n",
    "\t\"gen_hidden_dim\":[200],\n",
    "\t\"gen_num_layers\":[3],\n",
    "\t\"dis_hidden_dim\":[200],\n",
    "\t\"dis_num_layers\":[3],\n",
    "\t\"dis_lstm_dim\":[200],\n",
    "\t\"lr\":[0.0002],\n",
    "\t\"noise\":[0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_search(Model):\n",
    "\tparameters = {}\n",
    "\tif Model == \"VGAN\":\n",
    "\t\tvariable = VGAN_variable\n",
    "\n",
    "\t# 创建一个存 parameter的dictionary,随机选择一个parameter\n",
    "\tfor param in variable.keys():\n",
    "\t\tparameters[param] = choice(variable[param])\n",
    "\treturn parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { \n",
    "        \"name\": \"modified_adult-VGAN-1hot-norm\",\n",
    "\t\t\"train\": \"modified_adult_train.csv\",\t\n",
    "\t\t\"sample\": \"modified_adult_train.csv\",\n",
    "\t\t\"gmm_cols\":[],\n",
    "\t\t\"normalize_cols\":[0,2,4,10,11,12],\n",
    "\t\t\"one-hot_cols\":[1,3,5,6,7,8,9,13,14],\n",
    "\t\t\"ordinal_cols\":[],\n",
    "\t\t\"model\": \"VGAN\",\n",
    "\t\t\"n_epochs\":20,\n",
    "\t\t\"steps_per_epoch\":10,\n",
    "\t\t\"n_search\": 5,\n",
    "\t\t\"rand_search\": \"yes\",\n",
    "\t\t\"train_method\":\"VTrain\"\n",
    "\t}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { \n",
    "        \"name\": \"modified_adult-VGAN-1hot-norm\",\n",
    "\t\t\"train\": \"modified_adult_train_sample.csv\",\t\n",
    "\t\t\"sample\": \"modified_adult_train_sample.csv\",\n",
    "\t\t\"gmm_cols\":[],\n",
    "\t\t\"normalize_cols\":[0,2,4,10,11,12],\n",
    "\t\t\"one-hot_cols\":[1,3,5,6,7,8,9,13,14],\n",
    "\t\t\"ordinal_cols\":[],\n",
    "\t\t\"model\": \"VGAN\",\n",
    "\t\t\"n_epochs\":20,\n",
    "\t\t\"steps_per_epoch\":10,\n",
    "\t\t\"n_search\": 5,\n",
    "\t\t\"rand_search\": \"yes\",\n",
    "\t\t\"train_method\":\"VTrain\"\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = { \n",
    "#         \"name\": \"modified_adult-VGAN-1hot-norm\",\n",
    "# \t\t\"train\": \"adult_train.csv\",\t\n",
    "# \t\t\"sample\": \"adult_train.csv\",\n",
    "# \t\t\"gmm_cols\":[],\n",
    "# \t\t\"normalize_cols\":[0,2,4,10,11,12],\n",
    "# \t\t\"one-hot_cols\":[1,3,5,6,7,8,9,13,14],\n",
    "# \t\t\"ordinal_cols\":[],\n",
    "# \t\t\"model\": \"VGAN\",\n",
    "# \t\t\"n_epochs\":10,\n",
    "# \t\t\"steps_per_epoch\":10,\n",
    "# \t\t\"n_search\": 5,\n",
    "# \t\t\"rand_search\": \"yes\",\n",
    "# \t\t\"train_method\":\"VTrain\"\n",
    "# \t}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(config[\"train\"])\n",
    "n_search = config[\"n_search\"]\n",
    "search = 1\n",
    "ratio = 0.9\n",
    "noise = 0.1\n",
    "path = \"generated/\"\n",
    "fields = []\n",
    "col_type = []\n",
    "\n",
    "\n",
    "\n",
    "for i, col in enumerate(list(train)):\n",
    "\n",
    "    if i in config[\"normalize_cols\"]:\n",
    "        fields.append((col,NumericalField(\"normalize\")))\n",
    "        col_type.append(\"normalize\")\n",
    "    elif i in config[\"gmm_cols\"]:\n",
    "        fields.append((col, NumericalField(\"gmm\", n=5)))\n",
    "        col_type.append(\"gmm\")\n",
    "    elif i in config[\"one-hot_cols\"]:\n",
    "        fields.append((col, CategoricalField(\"one-hot\", noise=noise)))\n",
    "        col_type.append(\"one-hot\")\n",
    "    elif i in config[\"ordinal_cols\"]:\n",
    "        fields.append((col, CategoricalField(\"dict\")))\n",
    "        col_type.append(\"ordinal\")\n",
    "    else:\n",
    "        fields.append((col, CategoricalField(\"binary\",noise=noise)))\n",
    "        col_type.append(\"binary\")\n",
    "\n",
    "\n",
    "# print(\"train row : {}\".format(len(trn)))\n",
    "# print(\"sample row: {}\".format(len(samp)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, samp = Dataset.split(\n",
    "    fields = fields,\n",
    "    path = \".\",\n",
    "    train = config[\"train\"],\n",
    "    validation = config[\"sample\"],\n",
    "    format = \"csv\",\n",
    "    valid_ratio=ratio\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', <data.field.NumericalField at 0x16bb63c2580>),\n",
       " ('workclass', <data.field.CategoricalField at 0x16bb63c26d0>),\n",
       " ('fnlwgt', <data.field.NumericalField at 0x16bb63c2910>),\n",
       " ('education', <data.field.CategoricalField at 0x16bb6380f70>),\n",
       " ('education-num', <data.field.NumericalField at 0x16bff45c5e0>),\n",
       " ('marital-status', <data.field.CategoricalField at 0x16bff45c6d0>),\n",
       " ('occupation', <data.field.CategoricalField at 0x16c23e810d0>),\n",
       " ('relationship', <data.field.CategoricalField at 0x16c23e812e0>),\n",
       " ('race', <data.field.CategoricalField at 0x16c23e81340>),\n",
       " ('sex', <data.field.CategoricalField at 0x16c23e81220>),\n",
       " ('capital-gain', <data.field.NumericalField at 0x16c23e81370>),\n",
       " ('capital-loss', <data.field.NumericalField at 0x16c23e81460>),\n",
       " ('hours-per-week', <data.field.NumericalField at 0x16c23e814c0>),\n",
       " ('native-country', <data.field.CategoricalField at 0x16c23e81520>),\n",
       " ('income', <data.field.CategoricalField at 0x16c23e81070>),\n",
       " ('label', <data.field.CategoricalField at 0x16c23e81130>)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn.learn_convert()\n",
    "samp.learn_convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ANACONDA\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "model = config[\"model\"]\n",
    "train_method = config[\"train_method\"]\n",
    "\n",
    "\n",
    "param = parameter_search(config[\"model\"])\n",
    "train_it, sample_it = Iterator.split(\n",
    "    batch_size = param[\"batch_size\"],\n",
    "    train = trn,\n",
    "    validation = samp,\n",
    "    sort_key = None,\n",
    "    shuffle = True,\n",
    "    labels = None,\n",
    "    square = False,\n",
    "    pad = None\n",
    ")\n",
    "x_dim = train_it.data.shape[1]\n",
    "col_dim = trn.col_dim\n",
    "col_ind = trn.col_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_dim = 0\n",
    "condition = False\n",
    "sample_times = 1\n",
    "KL = True\n",
    "GPU = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl(real, pred):\n",
    "    return torch.sum((torch.log(pred + 1e-4) - torch.log(real + 1e-4)) * pred)\n",
    "\n",
    "def KL_Loss(x_fake, x_real, col_type, col_dim):\n",
    "    kl = 0.0\n",
    "    sta = 0\n",
    "    end = 0\n",
    "    for i in range(len(col_type)):\n",
    "        dim = col_dim[i]\n",
    "        sta = end\n",
    "        end = sta+dim\n",
    "        fakex = x_fake[:,sta:end]\n",
    "        realx = x_real[:,sta:end]\n",
    "        if col_type[i] == \"gmm\":\n",
    "            fake2 = fakex[:,1:]\n",
    "            real2 = realx[:,1:]\n",
    "            # column sum\n",
    "            dist = torch.sum(fake2, dim=0)\n",
    "\n",
    "            dist = dist / torch.sum(dist)\n",
    "            real = torch.sum(real2, dim=0)\n",
    "            real = real / torch.sum(real)\n",
    "            kl += compute_kl(real, dist)\n",
    "        else:\n",
    "            dist = torch.sum(fakex, dim=0)\n",
    "            dist = dist / torch.sum(dist)\n",
    "            \n",
    "            real = torch.sum(realx, dim=0)\n",
    "            real = real / torch.sum(real)\n",
    "            \n",
    "            kl += compute_kl(real, dist)\n",
    "    return kl\n",
    "\n",
    "\n",
    "def mean_Loss(x_fake, x_real, col_type, col_dim):\n",
    "    mean = 0.0\n",
    "    sta = 0\n",
    "    end = 0\n",
    "    for i in range(len(col_type)):\n",
    "        dim = col_dim[i]\n",
    "        sta = end\n",
    "        end = sta+dim\n",
    "        fakex = x_fake[:,sta:end]\n",
    "        realx = x_real[:,sta:end]\n",
    "        if col_type[i] == \"gmm\":\n",
    "            fake2 = fakex[:,1:]\n",
    "            real2 = realx[:,1:]\n",
    "            dist = torch.mean(fake2, dim=0)\n",
    "            dist = dist / torch.sum(dist)\n",
    "            real = torch.mean(real2, dim=0)\n",
    "            real = real / torch.sum(real)\n",
    "            mean += torch.sum(abs(real - dist))\n",
    "        else:\n",
    "            dist = torch.mean(fakex, dim=0)\n",
    "            dist = dist / torch.sum(dist)\n",
    "            \n",
    "            real = torch.mean(realx, dim=0)\n",
    "            real = real / torch.sum(real)\n",
    "            mean += torch.sum(abs(real - dist))\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(data,dataset):\n",
    "    samples = data.reshape(data.shape[0], -1)\n",
    "    samples = samples[:,:dataset.dim]\n",
    "    samples = samples.cpu()\n",
    "    sample_table = dataset.reverse(samples.detach().numpy())\n",
    "    df = pd.DataFrame(sample_table,columns=dataset.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Keras==2.3.1 in e:\\anaconda\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in e:\\anaconda\\lib\\site-packages (from Keras==2.3.1) (1.19.3)\n",
      "Requirement already satisfied: scipy>=0.14 in e:\\anaconda\\lib\\site-packages (from Keras==2.3.1) (1.4.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in e:\\anaconda\\lib\\site-packages (from Keras==2.3.1) (1.0.8)\n",
      "Requirement already satisfied: h5py in e:\\anaconda\\lib\\site-packages (from Keras==2.3.1) (2.10.0)\n",
      "Requirement already satisfied: six>=1.9.0 in e:\\anaconda\\lib\\site-packages (from Keras==2.3.1) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in e:\\anaconda\\lib\\site-packages (from Keras==2.3.1) (1.1.2)\n",
      "Requirement already satisfied: pyyaml in e:\\anaconda\\lib\\site-packages (from Keras==2.3.1) (5.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.2; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'E:\\ANACONDA\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install \"Keras==2.3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "118/118 [==============================] - 1s 3ms/step - loss: 5048.9971\n",
      "Epoch 2/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 5016.8115\n",
      "Epoch 3/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4950.3779\n",
      "Epoch 4/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4837.4399\n",
      "Epoch 5/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4666.3638\n",
      "Epoch 6/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4424.5938\n",
      "Epoch 7/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4100.8799\n",
      "Epoch 8/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 3683.5574\n",
      "Epoch 9/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 3160.9885\n",
      "Epoch 10/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2520.7329\n",
      "Epoch 11/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 1751.8827\n",
      "Epoch 12/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 846.6656\n",
      "Epoch 13/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 116.6701\n",
      "Epoch 14/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 75.2849\n",
      "Epoch 15/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 71.4862\n",
      "Epoch 16/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 67.8011\n",
      "Epoch 17/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 64.0594\n",
      "Epoch 18/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 60.2863\n",
      "Epoch 19/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 56.6085\n",
      "Epoch 20/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 52.7482\n",
      "Epoch 21/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 49.0429\n",
      "Epoch 22/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 45.2250\n",
      "Epoch 23/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 41.5543\n",
      "Epoch 24/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 37.7232\n",
      "Epoch 25/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 33.8934\n",
      "Epoch 26/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 30.0436\n",
      "Epoch 27/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 26.2404\n",
      "Epoch 28/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 22.4073\n",
      "Epoch 29/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 18.5989\n",
      "Epoch 30/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 15.0221\n",
      "Epoch 31/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 11.6466\n",
      "Epoch 32/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 8.8495\n",
      "Epoch 33/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 6.9378\n",
      "Epoch 34/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 5.7809\n",
      "Epoch 35/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 5.4010\n",
      "Epoch 36/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 5.1199\n",
      "Epoch 37/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.9626\n",
      "Epoch 38/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.8785\n",
      "Epoch 39/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.7946\n",
      "Epoch 40/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.7450\n",
      "Epoch 41/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.7112\n",
      "Epoch 42/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6834\n",
      "Epoch 43/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.7015\n",
      "Epoch 44/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6670\n",
      "Epoch 45/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6414\n",
      "Epoch 46/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6679\n",
      "Epoch 47/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6568\n",
      "Epoch 48/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6442\n",
      "Epoch 49/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6347\n",
      "Epoch 50/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6620\n",
      "Epoch 51/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6416\n",
      "Epoch 52/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6702\n",
      "Epoch 53/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6514\n",
      "Epoch 54/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6541\n",
      "Epoch 55/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6581\n",
      "Epoch 56/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6570\n",
      "Epoch 57/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6461\n",
      "Epoch 58/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6475\n",
      "Epoch 59/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6536\n",
      "Epoch 60/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6580\n",
      "Epoch 61/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6342\n",
      "Epoch 62/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6157\n",
      "Epoch 63/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6686\n",
      "Epoch 64/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6637\n",
      "Epoch 65/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6603\n",
      "Epoch 66/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6602\n",
      "Epoch 67/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6632\n",
      "Epoch 68/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6580\n",
      "Epoch 69/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6615\n",
      "Epoch 70/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6560\n",
      "Epoch 71/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6574\n",
      "Epoch 72/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6654\n",
      "Epoch 73/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6681\n",
      "Epoch 74/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6624\n",
      "Epoch 75/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6603\n",
      "Epoch 76/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6593\n",
      "Epoch 77/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6574\n",
      "Epoch 78/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6579\n",
      "Epoch 79/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6630\n",
      "Epoch 80/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6610\n",
      "Epoch 81/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6574\n",
      "Epoch 82/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6642\n",
      "Epoch 83/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6573\n",
      "Epoch 84/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6537\n",
      "Epoch 85/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6578\n",
      "Epoch 86/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6515\n",
      "Epoch 87/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6493\n",
      "Epoch 88/100\n",
      "118/118 [==============================] - 0s 4ms/step - loss: 4.6481\n",
      "Epoch 89/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6591\n",
      "Epoch 90/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6556\n",
      "Epoch 91/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6551\n",
      "Epoch 92/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6569\n",
      "Epoch 93/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6494\n",
      "Epoch 94/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6635\n",
      "Epoch 95/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6633\n",
      "Epoch 96/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6618\n",
      "Epoch 97/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6574\n",
      "Epoch 98/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6535\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6602\n",
      "Epoch 100/100\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 4.6503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16c32998820>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "fd_data = pd.read_csv(\"modified_adult_train.csv\")\n",
    "A = fd_data[\"education-num\"]\n",
    "B = fd_data[\"capital-gain\"]\n",
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=[1]),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "  model.compile(loss='mae',\n",
    "                optimizer=optimizer)\n",
    "  return model\n",
    "\n",
    "\n",
    "FD_model = build_model()\n",
    "\n",
    "FD_model.fit(\n",
    "  A, B,epochs=100,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def V_Train_modifed(t, path, sampleloader, G, D, \n",
    "epochs, lr, dataloader, z_dim, dataset, \n",
    "col_type, sample_times, itertimes = 100, \n",
    "steps_per_epoch = None, GPU=False, KL=True):\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    if GPU:\n",
    "        G.cuda()\n",
    "        D.cuda()\n",
    "    G.GPU = True\n",
    "    \n",
    "    D_optim = optim.Adam(D.parameters(), lr=lr, weight_decay=0.00001)\n",
    "    G_optim = optim.Adam(G.parameters(), lr=lr, weight_decay=0.00001)\n",
    "\n",
    "    # the default # of steps is the # of batches.\n",
    "\n",
    "    if steps_per_epoch is None:\n",
    "        steps_per_epoch = len(dataloader)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        it = 0\n",
    "        log = open(path+\"train_log_\"+str(t)+\".txt\",\"a+\")\n",
    "        log.write(\"-----------Epoch {}-----------\\n\".format(epoch))\n",
    "        log.close()\n",
    "        print(\"-----------Epoch {}-----------\\n\".format(epoch))\n",
    "        while it < steps_per_epoch:\n",
    "            \n",
    "            # batch 128, x_real.shape = [128,105]\n",
    "            for x_real in dataloader:\n",
    "                if GPU:\n",
    "                    x_real = x_real.cuda()\n",
    "\n",
    "                ''' train Discriminator '''\n",
    "                z = torch.randn(x_real.shape[0], z_dim)\n",
    "                \n",
    "                if GPU:\n",
    "                    z = z.cuda()   \n",
    "                x_fake = G(z)\n",
    "                \n",
    "                y_real = D(x_real)\n",
    "                y_fake = D(x_fake)\n",
    "\n",
    "                # df_fake = to_df(x_fake,dataset)\n",
    "                # A = df_fake[\"education-num\"].astype('float32')\n",
    "                # #predicted = model.predict(A)\n",
    "                # predicted = FD_model.predict(A)\n",
    "\n",
    "                #D_fd = sum(abs(predicted-df_fake[\"capital-gain\"]))/y_fake.shape[0]\n",
    "                D_fd = 0\n",
    "                #D_fd = sum(abs(predicted.flatten()-df_fake[\"capital-gain\"]))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "                # 生成 0 和 1，避免  Discriminator 对 Generator 压制\n",
    "                # D_Loss = -(torch.mean(y_real) - torch.mean(y_fake)) # wgan loss\n",
    "                fake_label = torch.zeros(y_fake.shape[0], 1)\n",
    "                real_label = np.ones([y_real.shape[0], 1])\n",
    "                # Avoid the suppress of Discriminator over Generator\n",
    "                real_label = real_label * 0.7 + np.random.uniform(0, 0.3, real_label.shape)\n",
    "                real_label = torch.from_numpy(real_label).float()\n",
    "                \n",
    "                if GPU:\n",
    "\n",
    "                    fake_label = fake_label.cuda()\n",
    "                    real_label = real_label.cuda()\n",
    "\n",
    " \n",
    "                    \n",
    "                D_Loss1 = F.binary_cross_entropy(y_real, real_label)\n",
    "                D_Loss2 = F.binary_cross_entropy(y_fake, fake_label)\n",
    "\n",
    "                D_Loss = D_Loss1 + D_Loss2 + D_fd\n",
    "                \n",
    "                G_optim.zero_grad()\n",
    "                D_optim.zero_grad()\n",
    "                D_Loss.backward()\n",
    "                D_optim.step()\n",
    "\n",
    "                ''' train Generator '''\n",
    "                # 生成 一批noisy\n",
    "                z = torch.randn(x_real.shape[0], z_dim)\n",
    "                if GPU:\n",
    "                    z = z.cuda()\n",
    "                \n",
    "                x_fake = G(z)\n",
    "                y_fake = D(x_fake)\n",
    "\n",
    "                \n",
    "\n",
    "                # df_fake = to_df(x_fake,dataset)\n",
    "                # A = df_fake[\"education-num\"].astype('float32')\n",
    "                # #predicted = model.predict(A)\n",
    "                # predicted = FD_model.predict(A)\n",
    "                # generated = df_fake[\"capital-gain\"].astype('float32')\n",
    "\n",
    "                G_fd = 0\n",
    "                # G_fd = sum(abs(predicted-generated))/y_fake.shape[0]\n",
    "                # print(\"Generator Loss: \",G_fd)\n",
    "                # log = open(path+\"train_log_\"+str(t)+\".txt\",\"a+\")\n",
    "                # log.write(\"Generator Loss: \\n\".format(G_fd))\n",
    "                # log.close()\n",
    "                #G_fd = sum(abs(predicted.flatten()-generated))\n",
    "                \n",
    "                \n",
    "\n",
    "                real_label = torch.ones(y_fake.shape[0], 1)\n",
    "                if GPU:\n",
    "                    real_label = real_label.cuda()\n",
    "\n",
    "                G_Loss1 = F.binary_cross_entropy(y_fake, real_label)\n",
    "                G_Loss2 = mean_Loss(x_fake, x_real, col_type, dataset.col_dim)\n",
    "                if KL:\n",
    "                    KL_loss = KL_Loss(x_fake, x_real, col_type, dataset.col_dim)\n",
    "                    G_Loss = G_Loss1 + KL_loss + G_fd + G_Loss2\n",
    "                    #G_Loss = G_Loss1 + KL_loss + G_fd\n",
    "                else:\n",
    "                    G_Loss = G_Loss1 + G_fd + G_Loss2\n",
    "                    #G_Loss = G_Loss1 + G_fd\n",
    "\n",
    "                G_optim.zero_grad()\n",
    "                D_optim.zero_grad()\n",
    "                G_Loss.backward()\n",
    "                G_optim.step()\n",
    "\n",
    "                it += 1\n",
    "\n",
    "                if it%itertimes == 0:\n",
    "                    log = open(path+\"train_log_\"+str(t)+\".txt\",\"a+\")\n",
    "                    log.write(\"iterator {}, D_Loss:{}, G_Loss:{}\\n\".format(it,D_Loss.data, G_Loss.data))\n",
    "                    log.close()\n",
    "                    print(\"iterator {}, D_Loss:{}, G_Loss:{}\\n\".format(it,D_Loss.data, G_Loss.data))\n",
    "                if it >= steps_per_epoch:\n",
    "                    G.eval()\n",
    "\n",
    "                    for time in range(sample_times):\n",
    "                        sample_data = None\n",
    "                        for x_real in sampleloader:\n",
    "                            z = torch.randn(x_real.shape[0], z_dim)\n",
    "                            if GPU:\n",
    "                                z = z.cuda()\n",
    "                            x_fake = G(z)\n",
    "                            samples = x_fake\n",
    "                            samples = samples.reshape(samples.shape[0], -1)\n",
    "                            samples = samples[:,:dataset.dim]\n",
    "                            samples = samples.cpu()\n",
    "                            sample_table = dataset.reverse(samples.detach().numpy())\n",
    "                            df = pd.DataFrame(sample_table,columns=dataset.columns)\n",
    "                            if sample_data is None:\n",
    "                                sample_data = df\n",
    "                            else:\n",
    "                                sample_data = sample_data.append(df)\n",
    "                        sample_data.to_csv(path+'sample_data_{}_{}_{}.csv'.format(t,epoch,time), index = None)\n",
    "                   # if GPU:\n",
    "                   #     G.cuda()\n",
    "                    #    G.GPU = True\n",
    "                    G.train()\n",
    "                    break\n",
    "    return G,D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Epoch 0-----------\n",
      "\n",
      "tensor([[0.5004],\n",
      "        [0.5130],\n",
      "        [0.5107],\n",
      "        [0.4889],\n",
      "        [0.5045],\n",
      "        [0.5131],\n",
      "        [0.5093],\n",
      "        [0.5057],\n",
      "        [0.5034],\n",
      "        [0.5022],\n",
      "        [0.5111],\n",
      "        [0.5065],\n",
      "        [0.5099],\n",
      "        [0.5062],\n",
      "        [0.4999],\n",
      "        [0.5190],\n",
      "        [0.4966],\n",
      "        [0.5027],\n",
      "        [0.5110],\n",
      "        [0.4910],\n",
      "        [0.5243],\n",
      "        [0.5125],\n",
      "        [0.5093],\n",
      "        [0.5033],\n",
      "        [0.5138],\n",
      "        [0.5123],\n",
      "        [0.5049],\n",
      "        [0.5086],\n",
      "        [0.4964],\n",
      "        [0.5017],\n",
      "        [0.4980],\n",
      "        [0.5104],\n",
      "        [0.5057],\n",
      "        [0.5129],\n",
      "        [0.5030],\n",
      "        [0.5049],\n",
      "        [0.4988],\n",
      "        [0.5039],\n",
      "        [0.5021],\n",
      "        [0.5230],\n",
      "        [0.5128],\n",
      "        [0.5043],\n",
      "        [0.5126],\n",
      "        [0.5222],\n",
      "        [0.5076],\n",
      "        [0.5114],\n",
      "        [0.4881],\n",
      "        [0.4938],\n",
      "        [0.5032],\n",
      "        [0.5143],\n",
      "        [0.5088],\n",
      "        [0.5047],\n",
      "        [0.5092],\n",
      "        [0.5128],\n",
      "        [0.4992],\n",
      "        [0.5083],\n",
      "        [0.5057],\n",
      "        [0.5131],\n",
      "        [0.5131],\n",
      "        [0.5048],\n",
      "        [0.5035],\n",
      "        [0.4870],\n",
      "        [0.5119],\n",
      "        [0.5074]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-c42e129a2f53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVGAN_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"z_dim\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"gen_hidden_dim\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"gen_num_layers\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVGAN_discriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dis_hidden_dim\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dis_num_layers\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mV_Train_modifed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"its_sample/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_it\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m700\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"lr\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_it\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"z_dim\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_times\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mitertimes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"steps_per_epoch\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mGPU\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGPU\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mKL\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mKL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-ca3d34a89cb2>\u001b[0m in \u001b[0;36mV_Train_modifed\u001b[1;34m(t, path, sampleloader, G, D, epochs, lr, dataloader, z_dim, dataset, col_type, sample_times, itertimes, steps_per_epoch, GPU, KL)\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[0mD_Loss1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0mD_Loss2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_fake\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ANACONDA\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;31m# All strings are unicode in Python 3.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ANACONDA\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_str_intern\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\ANACONDA\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m_str_intern\u001b[1;34m(inp)\u001b[0m\n\u001b[0;32m    379\u001b[0m                     \u001b[0mtensor_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m                     \u001b[0mtensor_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ANACONDA\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[1;34m(self, indent)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_formatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimag_formatter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ANACONDA\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0mnonzero_finite_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mtensor_view\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mne\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnonzero_finite_vals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "gen = VGAN_generator(param[\"z_dim\"], param[\"gen_hidden_dim\"], x_dim, param[\"gen_num_layers\"], col_type, col_ind, condition=condition,c_dim=c_dim)\n",
    "dis = VGAN_discriminator(x_dim, param[\"dis_hidden_dim\"], param[\"dis_num_layers\"],condition,c_dim)\n",
    "V_Train_modifed(search, \"its_sample/\", sample_it, gen, dis, 700, param[\"lr\"], train_it, param[\"z_dim\"], trn, col_type, sample_times,itertimes = 100, steps_per_epoch = config[\"steps_per_epoch\"],GPU=GPU,KL=KL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def V_Train_modifed_origin(t, path, sampleloader, G, D, \n",
    "epochs, lr, dataloader, z_dim, dataset, \n",
    "col_type, sample_times, itertimes = 100, \n",
    "steps_per_epoch = None, GPU=False, KL=True):\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    if GPU:\n",
    "        G.cuda()\n",
    "        D.cuda()\n",
    "    G.GPU = True\n",
    "    \n",
    "    D_optim = optim.Adam(D.parameters(), lr=lr, weight_decay=0.00001)\n",
    "    G_optim = optim.Adam(G.parameters(), lr=lr, weight_decay=0.00001)\n",
    "\n",
    "    # the default # of steps is the # of batches.\n",
    "\n",
    "    if steps_per_epoch is None:\n",
    "        steps_per_epoch = len(dataloader)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        it = 0\n",
    "        log = open(path+\"train_log_\"+str(t)+\".txt\",\"a+\")\n",
    "        log.write(\"-----------Epoch {}-----------\\n\".format(epoch))\n",
    "        log.close()\n",
    "        print(\"-----------Epoch {}-----------\\n\".format(epoch))\n",
    "        while it < steps_per_epoch:\n",
    "            \n",
    "            # batch 128, x_real.shape = [128,105]\n",
    "            for x_real in dataloader:\n",
    "                if GPU:\n",
    "                    x_real = x_real.cuda()\n",
    "\n",
    "                ''' train Discriminator '''\n",
    "                z = torch.randn(x_real.shape[0], z_dim)\n",
    "                \n",
    "                if GPU:\n",
    "                    z = z.cuda()   \n",
    "                x_fake = G(z)\n",
    "                \n",
    "                y_real = D(x_real)\n",
    "                y_fake = D(x_fake)\n",
    "\n",
    "                df_fake = to_df(x_fake,dataset)\n",
    "                A = df_fake[\"education-num\"].astype('float32')\n",
    "                #predicted = model.predict(A)\n",
    "                predicted = FD_model.predict(A)\n",
    "        \n",
    "\n",
    "\n",
    "                # 生成 0 和 1，避免  Discriminator 对 Generator 压制\n",
    "                # D_Loss = -(torch.mean(y_real) - torch.mean(y_fake)) # wgan loss\n",
    "                fake_label = torch.zeros(y_fake.shape[0], 1)\n",
    "                real_label = np.ones([y_real.shape[0], 1])\n",
    "                # Avoid the suppress of Discriminator over Generator\n",
    "                real_label = real_label * 0.7 + np.random.uniform(0, 0.3, real_label.shape)\n",
    "                real_label = torch.from_numpy(real_label).float()\n",
    "                \n",
    "                if GPU:\n",
    "\n",
    "                    fake_label = fake_label.cuda()\n",
    "                    real_label = real_label.cuda()\n",
    "\n",
    " \n",
    "                    \n",
    "                D_Loss1 = F.binary_cross_entropy(y_real, real_label)\n",
    "                D_Loss2 = F.binary_cross_entropy(y_fake, fake_label)\n",
    "\n",
    "                D_Loss = D_Loss1 + D_Loss2\n",
    "                \n",
    "                G_optim.zero_grad()\n",
    "                D_optim.zero_grad()\n",
    "                D_Loss.backward()\n",
    "                D_optim.step()\n",
    "\n",
    "                ''' train Generator '''\n",
    "                # 生成 一批noisy\n",
    "                z = torch.randn(x_real.shape[0], z_dim)\n",
    "                if GPU:\n",
    "                    z = z.cuda()\n",
    "                \n",
    "                x_fake = G(z)\n",
    "                y_fake = D(x_fake)\n",
    "\n",
    "                                \n",
    "\n",
    "                real_label = torch.ones(y_fake.shape[0], 1)\n",
    "                if GPU:\n",
    "                    real_label = real_label.cuda()\n",
    "\n",
    "                G_Loss1 = F.binary_cross_entropy(y_fake, real_label)\n",
    "                G_Loss2 = mean_Loss(x_fake, x_real, col_type, dataset.col_dim)\n",
    "                if KL:\n",
    "                    KL_loss = KL_Loss(x_fake, x_real, col_type, dataset.col_dim)\n",
    "                    #G_Loss = G_Loss1 + KL_loss + G_fd + G_Loss2\n",
    "                    G_Loss = G_Loss1 + KL_loss\n",
    "                else:\n",
    "                    #G_Loss = G_Loss1 + G_fd + G_Loss2\n",
    "                    G_Loss = G_Loss1\n",
    "\n",
    "                G_optim.zero_grad()\n",
    "                D_optim.zero_grad()\n",
    "                G_Loss.backward()\n",
    "                G_optim.step()\n",
    "\n",
    "                it += 1\n",
    "\n",
    "                if it%itertimes == 0:\n",
    "                    log = open(path+\"train_log_\"+str(t)+\".txt\",\"a+\")\n",
    "                    log.write(\"iterator {}, D_Loss:{}, G_Loss:{}\\n\".format(it,D_Loss.data, G_Loss.data))\n",
    "                    log.close()\n",
    "                    print(\"iterator {}, D_Loss:{}, G_Loss:{}\\n\".format(it,D_Loss.data, G_Loss.data))\n",
    "                if it >= steps_per_epoch:\n",
    "                    G.eval()\n",
    "\n",
    "                    for time in range(sample_times):\n",
    "                        sample_data = None\n",
    "                        for x_real in sampleloader:\n",
    "                            z = torch.randn(x_real.shape[0], z_dim)\n",
    "                            if GPU:\n",
    "                                z = z.cuda()\n",
    "                            x_fake = G(z)\n",
    "                            samples = x_fake\n",
    "                            samples = samples.reshape(samples.shape[0], -1)\n",
    "                            samples = samples[:,:dataset.dim]\n",
    "                            samples = samples.cpu()\n",
    "                            sample_table = dataset.reverse(samples.detach().numpy())\n",
    "                            df = pd.DataFrame(sample_table,columns=dataset.columns)\n",
    "                            if sample_data is None:\n",
    "                                sample_data = df\n",
    "                            else:\n",
    "                                sample_data = sample_data.append(df)\n",
    "                        sample_data.to_csv(path+'sample_data_{}_{}_{}.csv'.format(t,epoch,time), index = None)\n",
    "                   # if GPU:\n",
    "                   #     G.cuda()\n",
    "                    #    G.GPU = True\n",
    "                    G.train()\n",
    "                    break\n",
    "    return G,D\n",
    "\n",
    "\n",
    "gen = VGAN_generator(param[\"z_dim\"], param[\"gen_hidden_dim\"], x_dim, param[\"gen_num_layers\"], col_type, col_ind, condition=condition,c_dim=c_dim)\n",
    "dis = VGAN_discriminator(x_dim, param[\"dis_hidden_dim\"], param[\"dis_num_layers\"],condition,c_dim)\n",
    "V_Train_modifed_origin(search, \"origin_loss_sample/\", sample_it, gen, dis, 700, param[\"lr\"], train_it, param[\"z_dim\"], trn, col_type, sample_times,itertimes = 100, steps_per_epoch = config[\"steps_per_epoch\"],GPU=GPU,KL=KL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def V_Train_modifed_mean(t, path, sampleloader, G, D, \n",
    "epochs, lr, dataloader, z_dim, dataset, \n",
    "col_type, sample_times, itertimes = 100, \n",
    "steps_per_epoch = None, GPU=False, KL=True):\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    if GPU:\n",
    "        G.cuda()\n",
    "        D.cuda()\n",
    "    G.GPU = True\n",
    "    \n",
    "    D_optim = optim.Adam(D.parameters(), lr=lr, weight_decay=0.00001)\n",
    "    G_optim = optim.Adam(G.parameters(), lr=lr, weight_decay=0.00001)\n",
    "\n",
    "    # the default # of steps is the # of batches.\n",
    "\n",
    "    if steps_per_epoch is None:\n",
    "        steps_per_epoch = len(dataloader)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        it = 0\n",
    "        log = open(path+\"train_log_\"+str(t)+\".txt\",\"a+\")\n",
    "        log.write(\"-----------Epoch {}-----------\\n\".format(epoch))\n",
    "        log.close()\n",
    "        print(\"-----------Epoch {}-----------\\n\".format(epoch))\n",
    "        while it < steps_per_epoch:\n",
    "            \n",
    "            # batch 128, x_real.shape = [128,105]\n",
    "            for x_real in dataloader:\n",
    "                if GPU:\n",
    "                    x_real = x_real.cuda()\n",
    "\n",
    "                ''' train Discriminator '''\n",
    "                z = torch.randn(x_real.shape[0], z_dim)\n",
    "                \n",
    "                if GPU:\n",
    "                    z = z.cuda()   \n",
    "                x_fake = G(z)\n",
    "                \n",
    "                y_real = D(x_real)\n",
    "                y_fake = D(x_fake)\n",
    "\n",
    "                df_fake = to_df(x_fake,dataset)\n",
    "                A = df_fake[\"education-num\"].astype('float32')\n",
    "                #predicted = model.predict(A)\n",
    "                predicted = FD_model.predict(A)\n",
    "        \n",
    "\n",
    "\n",
    "                # 生成 0 和 1，避免  Discriminator 对 Generator 压制\n",
    "                # D_Loss = -(torch.mean(y_real) - torch.mean(y_fake)) # wgan loss\n",
    "                fake_label = torch.zeros(y_fake.shape[0], 1)\n",
    "                real_label = np.ones([y_real.shape[0], 1])\n",
    "                # Avoid the suppress of Discriminator over Generator\n",
    "                real_label = real_label * 0.7 + np.random.uniform(0, 0.3, real_label.shape)\n",
    "                real_label = torch.from_numpy(real_label).float()\n",
    "                \n",
    "                if GPU:\n",
    "\n",
    "                    fake_label = fake_label.cuda()\n",
    "                    real_label = real_label.cuda()\n",
    "\n",
    " \n",
    "                    \n",
    "                D_Loss1 = F.binary_cross_entropy(y_real, real_label)\n",
    "                D_Loss2 = F.binary_cross_entropy(y_fake, fake_label)\n",
    "\n",
    "                D_Loss = D_Loss1 + D_Loss2\n",
    "                \n",
    "                G_optim.zero_grad()\n",
    "                D_optim.zero_grad()\n",
    "                D_Loss.backward()\n",
    "                D_optim.step()\n",
    "\n",
    "                ''' train Generator '''\n",
    "                # 生成 一批noisy\n",
    "                z = torch.randn(x_real.shape[0], z_dim)\n",
    "                if GPU:\n",
    "                    z = z.cuda()\n",
    "                \n",
    "                x_fake = G(z)\n",
    "                y_fake = D(x_fake)\n",
    "\n",
    "                                \n",
    "\n",
    "                real_label = torch.ones(y_fake.shape[0], 1)\n",
    "                if GPU:\n",
    "                    real_label = real_label.cuda()\n",
    "\n",
    "                G_Loss1 = F.binary_cross_entropy(y_fake, real_label)\n",
    "                G_Loss2 = mean_Loss(x_fake, x_real, col_type, dataset.col_dim)\n",
    "                if KL:\n",
    "                    KL_loss = KL_Loss(x_fake, x_real, col_type, dataset.col_dim)\n",
    "                    #G_Loss = G_Loss1 + KL_loss + G_fd + G_Loss2\n",
    "                    G_Loss = G_Loss1 + KL_loss + G_Loss2\n",
    "                else:\n",
    "                    #G_Loss = G_Loss1 + G_fd + G_Loss2\n",
    "                    G_Loss = G_Loss1 + G_Loss2\n",
    "\n",
    "                G_optim.zero_grad()\n",
    "                D_optim.zero_grad()\n",
    "                G_Loss.backward()\n",
    "                G_optim.step()\n",
    "\n",
    "                it += 1\n",
    "\n",
    "                if it%itertimes == 0:\n",
    "                    log = open(path+\"train_log_\"+str(t)+\".txt\",\"a+\")\n",
    "                    log.write(\"iterator {}, D_Loss:{}, G_Loss:{}\\n\".format(it,D_Loss.data, G_Loss.data))\n",
    "                    log.close()\n",
    "                    print(\"iterator {}, D_Loss:{}, G_Loss:{}\\n\".format(it,D_Loss.data, G_Loss.data))\n",
    "                if it >= steps_per_epoch:\n",
    "                    G.eval()\n",
    "\n",
    "                    for time in range(sample_times):\n",
    "                        sample_data = None\n",
    "                        for x_real in sampleloader:\n",
    "                            z = torch.randn(x_real.shape[0], z_dim)\n",
    "                            if GPU:\n",
    "                                z = z.cuda()\n",
    "                            x_fake = G(z)\n",
    "                            samples = x_fake\n",
    "                            samples = samples.reshape(samples.shape[0], -1)\n",
    "                            samples = samples[:,:dataset.dim]\n",
    "                            samples = samples.cpu()\n",
    "                            sample_table = dataset.reverse(samples.detach().numpy())\n",
    "                            df = pd.DataFrame(sample_table,columns=dataset.columns)\n",
    "                            if sample_data is None:\n",
    "                                sample_data = df\n",
    "                            else:\n",
    "                                sample_data = sample_data.append(df)\n",
    "                        sample_data.to_csv(path+'sample_data_{}_{}_{}.csv'.format(t,epoch,time), index = None)\n",
    "                   # if GPU:\n",
    "                   #     G.cuda()\n",
    "                    #    G.GPU = True\n",
    "                    G.train()\n",
    "                    break\n",
    "    return G,D\n",
    "\n",
    "\n",
    "gen = VGAN_generator(param[\"z_dim\"], param[\"gen_hidden_dim\"], x_dim, param[\"gen_num_layers\"], col_type, col_ind, condition=condition,c_dim=c_dim)\n",
    "dis = VGAN_discriminator(x_dim, param[\"dis_hidden_dim\"], param[\"dis_num_layers\"],condition,c_dim)\n",
    "V_Train_modifed_mean(search, \"mean_loss_sample/\", sample_it, gen, dis, 700, param[\"lr\"], train_it, param[\"z_dim\"], trn, col_type, sample_times,itertimes = 100, steps_per_epoch = config[\"steps_per_epoch\"],GPU=GPU,KL=KL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def V_Train_modifed(t, path, sampleloader, G, D, \n",
    "epochs, lr, dataloader, z_dim, dataset, \n",
    "col_type, sample_times, itertimes = 100, \n",
    "steps_per_epoch = None, GPU=False, KL=True):\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    if GPU:\n",
    "        G.cuda()\n",
    "        D.cuda()\n",
    "    G.GPU = True\n",
    "    \n",
    "    D_optim = optim.Adam(D.parameters(), lr=lr, weight_decay=0.00001)\n",
    "    G_optim = optim.Adam(G.parameters(), lr=lr, weight_decay=0.00001)\n",
    "\n",
    "    # the default # of steps is the # of batches.\n",
    "\n",
    "    if steps_per_epoch is None:\n",
    "        steps_per_epoch = len(dataloader)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        it = 0\n",
    "        log = open(path+\"train_log_\"+str(t)+\".txt\",\"a+\")\n",
    "        log.write(\"-----------Epoch {}-----------\\n\".format(epoch))\n",
    "        log.close()\n",
    "        print(\"-----------Epoch {}-----------\\n\".format(epoch))\n",
    "        while it < steps_per_epoch:\n",
    "            \n",
    "            # batch 128, x_real.shape = [128,105]\n",
    "            for x_real in dataloader:\n",
    "                if GPU:\n",
    "                    x_real = x_real.cuda()\n",
    "\n",
    "                ''' train Discriminator '''\n",
    "                z = torch.randn(x_real.shape[0], z_dim)\n",
    "                \n",
    "                if GPU:\n",
    "                    z = z.cuda()   \n",
    "                x_fake = G(z)\n",
    "                \n",
    "                y_real = D(x_real)\n",
    "                y_fake = D(x_fake)\n",
    "\n",
    "                df_fake = to_df(x_fake,dataset)\n",
    "                A = df_fake[\"education-num\"].astype('float32')\n",
    "                #predicted = model.predict(A)\n",
    "                predicted = FD_model.predict(A)\n",
    "\n",
    "                D_fd = sum(abs(predicted-df_fake[\"capital-gain\"]))/y_fake.shape[0]\n",
    "                #D_fd = sum(abs(predicted.flatten()-df_fake[\"capital-gain\"]))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "                # 生成 0 和 1，避免  Discriminator 对 Generator 压制\n",
    "                # D_Loss = -(torch.mean(y_real) - torch.mean(y_fake)) # wgan loss\n",
    "                fake_label = torch.zeros(y_fake.shape[0], 1)\n",
    "                real_label = np.ones([y_real.shape[0], 1])\n",
    "                # Avoid the suppress of Discriminator over Generator\n",
    "                real_label = real_label * 0.7 + np.random.uniform(0, 0.3, real_label.shape)\n",
    "                real_label = torch.from_numpy(real_label).float()\n",
    "                \n",
    "                if GPU:\n",
    "\n",
    "                    fake_label = fake_label.cuda()\n",
    "                    real_label = real_label.cuda()\n",
    "\n",
    " \n",
    "                    \n",
    "                D_Loss1 = F.binary_cross_entropy(y_real, real_label)\n",
    "                D_Loss2 = F.binary_cross_entropy(y_fake, fake_label)\n",
    "\n",
    "                D_Loss = D_Loss1 + D_Loss2 + D_fd\n",
    "                \n",
    "                G_optim.zero_grad()\n",
    "                D_optim.zero_grad()\n",
    "                D_Loss.backward()\n",
    "                D_optim.step()\n",
    "\n",
    "                ''' train Generator '''\n",
    "                # 生成 一批noisy\n",
    "                z = torch.randn(x_real.shape[0], z_dim)\n",
    "                if GPU:\n",
    "                    z = z.cuda()\n",
    "                \n",
    "                x_fake = G(z)\n",
    "                y_fake = D(x_fake)\n",
    "\n",
    "                \n",
    "\n",
    "                df_fake = to_df(x_fake,dataset)\n",
    "                A = df_fake[\"education-num\"].astype('float32')\n",
    "                #predicted = model.predict(A)\n",
    "                predicted = FD_model.predict(A)\n",
    "                generated = df_fake[\"capital-gain\"].astype('float32')\n",
    "\n",
    "\n",
    "                G_fd = sum(abs(predicted-generated))/y_fake.shape[0]\n",
    "                print(\"Generator Loss: \",G_fd)\n",
    "                log = open(path+\"train_log_\"+str(t)+\".txt\",\"a+\")\n",
    "                log.write(\"Generator Loss: \\n\".format(G_fd))\n",
    "                log.close()\n",
    "                #G_fd = sum(abs(predicted.flatten()-generated))\n",
    "                \n",
    "                \n",
    "\n",
    "                real_label = torch.ones(y_fake.shape[0], 1)\n",
    "                if GPU:\n",
    "                    real_label = real_label.cuda()\n",
    "\n",
    "                G_Loss1 = F.binary_cross_entropy(y_fake, real_label)\n",
    "                G_Loss2 = mean_Loss(x_fake, x_real, col_type, dataset.col_dim)\n",
    "                if KL:\n",
    "                    KL_loss = KL_Loss(x_fake, x_real, col_type, dataset.col_dim)\n",
    "                    G_Loss = G_Loss1 + KL_loss + G_fd\n",
    "                    #G_Loss = G_Loss1 + KL_loss + G_fd\n",
    "                else:\n",
    "                    G_Loss = G_Loss1 + G_fd\n",
    "                    #G_Loss = G_Loss1 + G_fd\n",
    "\n",
    "                G_optim.zero_grad()\n",
    "                D_optim.zero_grad()\n",
    "                G_Loss.backward()\n",
    "                G_optim.step()\n",
    "\n",
    "                it += 1\n",
    "\n",
    "                if it%itertimes == 0:\n",
    "                    log = open(path+\"train_log_\"+str(t)+\".txt\",\"a+\")\n",
    "                    log.write(\"iterator {}, D_Loss:{}, G_Loss:{}\\n\".format(it,D_Loss.data, G_Loss.data))\n",
    "                    log.close()\n",
    "                    print(\"iterator {}, D_Loss:{}, G_Loss:{}\\n\".format(it,D_Loss.data, G_Loss.data))\n",
    "                if it >= steps_per_epoch:\n",
    "                    G.eval()\n",
    "\n",
    "                    for time in range(sample_times):\n",
    "                        sample_data = None\n",
    "                        for x_real in sampleloader:\n",
    "                            z = torch.randn(x_real.shape[0], z_dim)\n",
    "                            if GPU:\n",
    "                                z = z.cuda()\n",
    "                            x_fake = G(z)\n",
    "                            samples = x_fake\n",
    "                            samples = samples.reshape(samples.shape[0], -1)\n",
    "                            samples = samples[:,:dataset.dim]\n",
    "                            samples = samples.cpu()\n",
    "                            sample_table = dataset.reverse(samples.detach().numpy())\n",
    "                            df = pd.DataFrame(sample_table,columns=dataset.columns)\n",
    "                            if sample_data is None:\n",
    "                                sample_data = df\n",
    "                            else:\n",
    "                                sample_data = sample_data.append(df)\n",
    "                        sample_data.to_csv(path+'sample_data_{}_{}_{}.csv'.format(t,epoch,time), index = None)\n",
    "                   # if GPU:\n",
    "                   #     G.cuda()\n",
    "                    #    G.GPU = True\n",
    "                    G.train()\n",
    "                    break\n",
    "    return G,D\n",
    "\n",
    "gen = VGAN_generator(param[\"z_dim\"], param[\"gen_hidden_dim\"], x_dim, param[\"gen_num_layers\"], col_type, col_ind, condition=condition,c_dim=c_dim)\n",
    "dis = VGAN_discriminator(x_dim, param[\"dis_hidden_dim\"], param[\"dis_num_layers\"],condition,c_dim)\n",
    "V_Train_modifed(search, \"fd_loss_sample/\", sample_it, gen, dis, 700, param[\"lr\"], train_it, param[\"z_dim\"], trn, col_type, sample_times,itertimes = 100, steps_per_epoch = config[\"steps_per_epoch\"],GPU=GPU,KL=KL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(data,dataset):\n",
    "    samples = data.reshape(data.shape[0], -1)\n",
    "    samples = samples[:,:dataset.dim]\n",
    "    samples = samples.cpu()\n",
    "    sample_table = dataset.reverse(samples.detach().numpy())\n",
    "    df = pd.DataFrame(sample_table,columns=dataset.columns)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "fd_data = pd.read_csv(\"modified_adult_train.csv\")\n",
    "A = fd_data[\"education-num\"]\n",
    "B = fd_data[\"capital-gain\"]\n",
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=[1]),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "  model.compile(loss='mae',\n",
    "                optimizer=optimizer)\n",
    "  return model\n",
    "\n",
    "\n",
    "FD_model = build_model()\n",
    "\n",
    "FD_model.fit(\n",
    "  A, B,epochs=100,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_data = pd.read_csv(\"modified_adult_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_norm=(fd_data[\"education-num\"]-fd_data[\"education-num\"].mean())/fd_data[\"education-num\"].std()\n",
    "B_norm=(fd_data[\"capital-gain\"]-fd_data[\"capital-gain\"].mean())/fd_data[\"capital-gain\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = fd_data[\"education-num\"]\n",
    "B = fd_data[\"capital-gain\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FD_autoencoder():\n",
    "    data = layers.Input(shape = (None, 1))\n",
    "\n",
    "    #encoded = layers.Conv1D(1,1,2)(data)\n",
    "    encoded = layers.Conv1DTranspose(1, 5, strides=2)(data)\n",
    "    decoded = layers.Conv1D(1, 5, strides=2)(encoded)\n",
    "    \n",
    "    #decoded = layers.Conv1DTranspose(1, 5, strides=2)(encoded)\n",
    "\n",
    "\n",
    "    encoder = keras.Model(data, encoded)\n",
    "    autoencoder = keras.Model(data, decoded)\n",
    "\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss=tf.keras.losses.MeanAbsoluteError())\n",
    "\n",
    "    return autoencoder#\n",
    "autoencoder= FD_autoencoder()\n",
    "optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "# autoencoder.compile(loss='mse',\n",
    "#                 optimizer=optimizer,\n",
    "#                 metrics=['mae', 'mse'])\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss=tf.keras.losses.MeanAbsoluteError())\n",
    "autoencoder.fit(\n",
    "  A, B,epochs=200,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=[1]),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "  model.compile(loss='mae',\n",
    "                optimizer=optimizer)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FD_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = FD_model.fit(\n",
    "  A, B,epochs=100,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FD_model.predict(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "612b56b8613cef43a0aa46ecdb75265b2a61f7d7fff48b249e1cc6885a9d0691"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
